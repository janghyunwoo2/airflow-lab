# 1\. 오리엔테이션 – Airflow는 왜 필요할까?

## 🎯 이 강의에서 배우는 것

이 강의의 목표는 다음과 같습니다.

- Apache Airflow의 **핵심 개념과 구조**를 이해한다
- 로컬 환경에서 **Airflow를 직접 설치하고 실행**해본다
- DAG을 작성하고, 주요 Operator를 **직접 만들어 실행**해본다
- 실무에서 바로 사용할 수 있는 **기본 ETL DAG 구조**를 완성한다

* * *

## ❓ 데이터 파이프라인의 현실적인 문제

- cron으로 여러 스크립트를 관리하다가→ 실행 순서가 꼬임
- 특정 배치가 실패했는데→ 어디서 실패했는지 로그를 찾기 어려움
- 전 단계가 실패했는데→ 다음 단계가 그냥 실행됨
- 매일/매시간 실행되는 작업의→ 상태를 한 눈에 보고 싶음

이런 문제들은 **작업이 많아질수록 더 심각해집니다.**

* * *

## 🤔 Cron vs Airflow

### Cron의 한계

- 작업 **의존성 표현이 어려움**
- 실패/재시도 관리가 불편함
- 모니터링 UI 없음
- 조건 분기 로직이 복잡해짐

### Airflow가 해결하는 것

- DAG으로 **의존성 명시**
- Task 단위 상태 관리
- 재시도, SLA, 알림 지원
- Web UI 기반 모니터링
- 코드 기반 워크플로우 관리

* * *

## 🔄 Airflow는 무엇인가?

Apache Airflow는 다음과 같은 도구입니다.

> **“코드로 워크플로우를 정의하고,  
> 이를 스케줄링하고,  
> 실행 상태를 관리하는 플랫폼”**

핵심 키워드:

- Workflow as Code
- DAG(Directed Acyclic Graph)
- Scheduler + Executor 구조
- 확장 가능한 Operator

* * *

## 🏢 실무에서 Airflow는 어디에 쓰일까?

대표적인 사용 사례:

- ETL 파이프라인
    - Raw → Clean → Aggregation
- 데이터 웨어하우스 적재
    - S3 → Redshift / BigQuery
- 머신러닝 파이프라인
    - Feature 생성 → Training → Validation
- 운영 자동화
    - 주기적 점검, 리포트 생성

* * *

## 🗺️ 오늘 강의의 전체 흐름

1. Airflow 기본 개념 이해
2. Docker 기반 Airflow 설치
3. DAG 기본 문법 실습
4. 주요 Operator 실습
5. 간단한 ETL DAG 만들기