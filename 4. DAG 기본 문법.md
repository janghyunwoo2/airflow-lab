# 4\. DAG 기본 문법

## 🎯 목표

- Airflow에서 DAG이 **어떻게 정의되는지** 이해한다
- DAG의 핵심 파라미터를 직접 설정해본다
- Task 의존성을 코드로 표현할 수 있다
- Airflow 3.x에서 권장하는 **TaskFlow API**를 사용해본다

* * *

## ⚠️ DAG은 “실행 코드”가 아니다

> **DAG 파일은 실행되는 코드가 아니라  
> “어떤 작업을, 어떤 순서로 실행할지”를 정의한 설정 파일이다.**

- DAG 파일은 **주기적으로 파싱**된다
- import 시점에 실행되는 코드는 주의해야 한다
- 무거운 연산, API 호출, DB 접근은 DAG 정의부에 두면 안 된다

* * *

## 📁 DAG 파일 위치

- 로컬: `./dags/`
- 컨테이너 내부: `/opt/airflow/dags`

> `dags/` 아래의 모든 `.py` 파일은 **DAG Processor**가 파싱한다

* * *

## ✍️ DAG 정의 방식

1. 클래식 방식 (`DAG` 객체)
2. **TaskFlow API (권장)**

* * *

## 🧩 DAG의 기본 구성 요소

### 필수

- `dag_id`
- `start_date`

### 자주 사용

- `schedule`
- `catchup`
- `tags`
- `default_args`

* * *

## 🧪 첫 번째 DAG: Hello World

**파일명: `dags/04_hello_dag.py`**  

```python
from airflow.decorators import dag, task
from datetime import datetime

@dag(
    dag_id="hello_world_dag",
    start_date=datetime(2024, 1, 1),
    schedule="@daily",
    catchup=False,
    tags=["example", "tutorial"],
)
def hello_world():

    @task
    def say_hello():
        print("Hello, Airflow!")

    say_hello()

hello_world()
```

* * *

## 🧩 DAG 파라미터 요약

- **dag\_id**: DAG를 식별하는 고유 ID
- **start\_date**: 스케줄 계산 기준(anchor)
- **schedule**: 실행 주기
- **catchup**: 과거 실행분 자동 실행 여부
- **tags**: UI 분류용 메타데이터

* * *

## 🧩 Task 정의 (`@task`)

- Python 함수 = Task
- 반환값은 자동으로 XCom 저장
- 실행 시점에만 코드가 실행됨

* * *

## 🔗 Task 의존성 설정 방법

Airflow에서는 Task 실행 순서를 **두 가지 방식**으로 표현할 수 있다.

* * *

## 🧪 Task 의존성 샘플 코드

### 방법 1️⃣ TaskFlow 방식 (함수 호출 기반)

**파일명: `dags/04_taskflow.py`**  

```python
from airflow.decorators import dag, task
from airflow.utils.log.logging_mixin import LoggingMixin
from datetime import datetime

log = LoggingMixin().log

@dag(
    dag_id="dependency_taskflow_demo",
    start_date=datetime(2024, 1, 1),
    schedule=None,
    catchup=False,
    tags=["example", "tutorial"],
)
def dependency_taskflow_demo():

    @task
    def extract():
        log.info("extract")
        return "raw"

    @task
    def transform(data: str):
        log.info("transform: %s", data)
        return data.upper()

    @task
    def load(result: str):
        log.info("load: %s", result)

    raw = extract()
    processed = transform(raw)
    load(processed)

dependency_taskflow_demo()
```

* * *

### 방법 2️⃣ `>>` 방식 (명시적 의존성)

**파일명: `dags/04_shift_operator.py`**  

```python
from airflow.decorators import dag
from airflow.operators.python import PythonOperator
from airflow.utils.log.logging_mixin import LoggingMixin
from datetime import datetime

log = LoggingMixin().log

def extract_fn(**context):
    log.info("extract")
    return "raw"

def transform_fn(**context):
    log.info("transform")
    return "RAW"

def load_fn(**context):
    log.info("load")

@dag(
    dag_id="dependency_shift_operator_demo",
    start_date=datetime(2024, 1, 1),
    schedule=None,
    catchup=False,
    tags=["example", "tutorial"],
)
def dependency_shift_operator_demo():

    extract = PythonOperator(task_id="extract", python_callable=extract_fn)
    transform = PythonOperator(task_id="transform", python_callable=transform_fn)
    load = PythonOperator(task_id="load", python_callable=load_fn)

    extract >> transform >> load

dependency_shift_operator_demo()
```

* * *

## ✏️ 두 방식의 차이 요약

- TaskFlow: 실행 순서 + 데이터 흐름을 동시에 표현 (권장)
- `>>` 방식: 실행 순서만 표현, 데이터 전달은 명시적 처리

* * *

## 🔗 다음 파트 예고

다음 파트에서는 **핵심 Operator 실습**을 진행한다.

<br>